{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-15T11:35:02.302835Z","iopub.execute_input":"2023-07-15T11:35:02.303554Z","iopub.status.idle":"2023-07-15T11:35:02.721857Z","shell.execute_reply.started":"2023-07-15T11:35:02.303516Z","shell.execute_reply":"2023-07-15T11:35:02.720992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install hugginface\n!pip install -U sentence-transformers\n!pip install seaborn\n!pip install -q -U keras-tuner","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-15T11:35:02.723562Z","iopub.execute_input":"2023-07-15T11:35:02.723937Z","iopub.status.idle":"2023-07-15T11:35:37.077076Z","shell.execute_reply.started":"2023-07-15T11:35:02.723913Z","shell.execute_reply":"2023-07-15T11:35:37.075983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-15T11:35:37.078488Z","iopub.execute_input":"2023-07-15T11:35:37.078772Z","iopub.status.idle":"2023-07-15T11:36:25.719700Z","shell.execute_reply.started":"2023-07-15T11:35:37.078745Z","shell.execute_reply":"2023-07-15T11:36:25.718620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-15T11:36:25.721919Z","iopub.execute_input":"2023-07-15T11:36:25.722189Z","iopub.status.idle":"2023-07-15T11:36:25.900180Z","shell.execute_reply.started":"2023-07-15T11:36:25.722165Z","shell.execute_reply":"2023-07-15T11:36:25.898832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[['premise', 'hypothesis', 'lang_abv', 'label']]\n#test = test[['premise', 'hypothesis', 'lang_abv']]\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:25.901495Z","iopub.execute_input":"2023-07-15T11:36:25.901809Z","iopub.status.idle":"2023-07-15T11:36:25.926531Z","shell.execute_reply.started":"2023-07-15T11:36:25.901783Z","shell.execute_reply":"2023-07-15T11:36:25.925592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n#distribuzione dei dati in base a lingua e sub distribuzione in base al label\nfig, ax = plt.subplots(figsize = (12,5))\n\ngraph1 = sns.countplot(data=train, x = \"lang_abv\", hue=\"label\")\n\n#set title\ngraph1.set_title('Distribution of Languages and Labels')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:25.927665Z","iopub.execute_input":"2023-07-15T11:36:25.927998Z","iopub.status.idle":"2023-07-15T11:36:28.162540Z","shell.execute_reply.started":"2023-07-15T11:36:25.927971Z","shell.execute_reply":"2023-07-15T11:36:28.161553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = train[train[\"lang_abv\"]==\"en\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:28.163809Z","iopub.execute_input":"2023-07-15T11:36:28.164101Z","iopub.status.idle":"2023-07-15T11:36:28.168172Z","shell.execute_reply.started":"2023-07-15T11:36:28.164074Z","shell.execute_reply":"2023-07-15T11:36:28.167097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#Utilizzato per Experiment 1 e 2\n#Script per bilanciare il dataset\nprint(\"------------------------------------\")\nprint(\"Distribuzione delle label prima del bilanciamento\")\nprint(train.value_counts(train['label']))\nprint(\"------------------------------------\")\nminLab= train.value_counts(train['label'])[1]\nnormal = train.value_counts(train['label'])[0]- minLab\ncontradictory = train.value_counts(train['label'])[2] - minLab\nprint(\"Minima quantita di lable: \", minLab, normal, contradictory)\n#Normalizzazione delle label al valore minimo 2166\nfor index, row in train.iterrows():\n    if (row['label']==0 and normal > 0):\n        train = train.drop(index, axis='index')\n        normal -= 1\n    elif(row['label']==2 and contradictory>0):\n        train = train.drop(index, axis='index')\n        contradictory -= 1\n        \nprint(\"------------------------------------\")  \nprint(\"Distribuzione delle label dopo il bilanciamento\")\nprint(train.value_counts(train['label']))\nprint(\"------------------------------------\")\n'''","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:28.169347Z","iopub.execute_input":"2023-07-15T11:36:28.169668Z","iopub.status.idle":"2023-07-15T11:36:28.182708Z","shell.execute_reply.started":"2023-07-15T11:36:28.169642Z","shell.execute_reply":"2023-07-15T11:36:28.181846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#ci sono pochi dati per tutte le lingue tranne l'inglese quindi restringo il training solo alla lingua dominante del dataset.\n#divido il dataset in train validation e test dato che sto riducendo solo ad inglese.\nX = train[['premise', 'hypothesis', 'lang_abv']]\nY = train [['label']]\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:28.183781Z","iopub.execute_input":"2023-07-15T11:36:28.184036Z","iopub.status.idle":"2023-07-15T11:36:28.373154Z","shell.execute_reply.started":"2023-07-15T11:36:28.184014Z","shell.execute_reply":"2023-07-15T11:36:28.372197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:28.376710Z","iopub.execute_input":"2023-07-15T11:36:28.377017Z","iopub.status.idle":"2023-07-15T11:36:28.557130Z","shell.execute_reply.started":"2023-07-15T11:36:28.376991Z","shell.execute_reply":"2023-07-15T11:36:28.556282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n#\"facebook/bart-large-mnli\" Experiment 2\n#facebook/bart-large Experiment 1\n#\"joeddav/xlm-roberta-large-xnli\" Experiment 3\ntokenizer = AutoTokenizer.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")\n\n#model = AutoModelForSequenceClassification.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:28.558074Z","iopub.execute_input":"2023-07-15T11:36:28.558340Z","iopub.status.idle":"2023-07-15T11:36:31.700118Z","shell.execute_reply.started":"2023-07-15T11:36:28.558317Z","shell.execute_reply":"2023-07-15T11:36:31.698726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer.tokenize(\"test tokenizer test test \")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:31.701643Z","iopub.execute_input":"2023-07-15T11:36:31.701970Z","iopub.status.idle":"2023-07-15T11:36:31.705818Z","shell.execute_reply.started":"2023-07-15T11:36:31.701944Z","shell.execute_reply":"2023-07-15T11:36:31.704858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEQ_LEN = 236  #max(train.astype('str').applymap(lambda x: len(x)).max())\n\ndef bert_encode(df, tokenizer):    \n    batch_premises = df['premise'].tolist()\n    batch_hypothesis = df['hypothesis'].tolist()\n\n    tokens = tokenizer(batch_premises, batch_hypothesis, max_length = SEQ_LEN,\n                   truncation=True, padding='max_length',\n                   add_special_tokens= True, return_attention_mask=True,\n                   return_tensors='tf') #return tf.constant \n    inputs = {\n          'input_ids': tokens['input_ids'], \n          'attention_mask': tokens['attention_mask'],\n          }\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:31.707005Z","iopub.execute_input":"2023-07-15T11:36:31.707315Z","iopub.status.idle":"2023-07-15T11:36:31.717159Z","shell.execute_reply.started":"2023-07-15T11:36:31.707288Z","shell.execute_reply":"2023-07-15T11:36:31.716327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(x_train, tokenizer)\ntest_input = bert_encode(x_test, tokenizer)\ntrain_input","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:36:31.718125Z","iopub.execute_input":"2023-07-15T11:36:31.718420Z","iopub.status.idle":"2023-07-15T11:36:33.197122Z","shell.execute_reply.started":"2023-07-15T11:36:31.718395Z","shell.execute_reply":"2023-07-15T11:36:33.195868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import regularizers\nfrom transformers import TFAutoModel\nimport tensorflow as tf\n#\"facebook/bart-large\"\n#\"facebook/bart-large-mnli\"\n#\"joeddav/xlm-roberta-large-xnli\"\ndef build_model():\n    transformer = TFAutoModel.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")\n    input_ids = tf.keras.Input(shape=(SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.Input(shape=(SEQ_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    embedding = transformer([input_ids, attention_mask])\n    inputs=[input_ids, attention_mask]\n    x = tf.keras.layers.GlobalAveragePooling1D()(embedding[0])\n    x = tf.keras.layers.Dense(units = 512, activation=tf.nn.relu)(x)\n    x = tf.keras.layers.Dense(units = 256, activation=tf.nn.relu)(x)\n    output = tf.keras.layers.Dense(3, activation='softmax')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    transformer.trainable = False\n    hp_learning_rate = 1e-5\n    model.compile(tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=['accuracy'])\n    return model ","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:49:32.176355Z","iopub.execute_input":"2023-07-15T11:49:32.176772Z","iopub.status.idle":"2023-07-15T11:49:32.186989Z","shell.execute_reply.started":"2023-07-15T11:49:32.176740Z","shell.execute_reply":"2023-07-15T11:49:32.185929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()     ","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:49:32.654868Z","iopub.execute_input":"2023-07-15T11:49:32.655147Z","iopub.status.idle":"2023-07-15T11:50:09.462991Z","shell.execute_reply.started":"2023-07-15T11:49:32.655123Z","shell.execute_reply":"2023-07-15T11:50:09.461788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_input, y_train , epochs = 20, batch_size=64, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:48:04.975742Z","iopub.execute_input":"2023-07-15T11:48:04.976072Z","iopub.status.idle":"2023-07-15T11:48:23.352964Z","shell.execute_reply.started":"2023-07-15T11:48:04.976044Z","shell.execute_reply":"2023-07-15T11:48:23.351350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result = model.evaluate(test_input, y_test)\nprint(\"[test loss, test accuracy]:\", eval_result)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:46:01.971108Z","iopub.execute_input":"2023-07-15T11:46:01.971541Z","iopub.status.idle":"2023-07-15T11:46:05.037935Z","shell.execute_reply.started":"2023-07-15T11:46:01.971508Z","shell.execute_reply":"2023-07-15T11:46:05.036697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"/kaggle/working/\"+'contradictory_classifier_roBERTa')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(20)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:46:10.972698Z","iopub.execute_input":"2023-07-15T11:46:10.973058Z","iopub.status.idle":"2023-07-15T11:46:11.476976Z","shell.execute_reply.started":"2023-07-15T11:46:10.973031Z","shell.execute_reply":"2023-07-15T11:46:11.475871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(y_pred):\n    prediction = np.empty(y_pred.shape[0], dtype=float)\n    for i in range(y_pred.shape[0]):\n        maxValue = max(y_pred[i])\n        itemIndex = np.where(y_pred[i] == maxValue)\n        prediction[i]= itemIndex[0][0]\n    prediction = pd.DataFrame(prediction, columns=['label'])\n    return prediction\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:46:12.100052Z","iopub.execute_input":"2023-07-15T11:46:12.100438Z","iopub.status.idle":"2023-07-15T11:46:12.107597Z","shell.execute_reply.started":"2023-07-15T11:46:12.100406Z","shell.execute_reply":"2023-07-15T11:46:12.106476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(test_input)\ny_pred = get_prediction(y_pred)\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = ['0','1','2'], \n                     columns = ['0','1','2'])\n\n#Plotting the confusion matrix\nplt.figure(figsize=(5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Confusion Matrix')\nplt.ylabel('Actal Values')\nplt.xlabel('Predicted Values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T11:45:59.594909Z","iopub.status.idle":"2023-07-15T11:45:59.595234Z","shell.execute_reply.started":"2023-07-15T11:45:59.595069Z","shell.execute_reply":"2023-07-15T11:45:59.595084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}